<!doctype html>
<html lang="zh-CN">
 <head> 
  <meta charset="utf-8"> 
  <link rel="canonical" href="https://blog.csdn.net/qq_39815222/article/details/142689795"> 
  <meta http-equiv="content-type" content="text/html; charset=utf-8"> 
  <meta name="renderer" content="webkit"> 
  <meta name="force-rendering" content="webkit"> 
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"> 
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no"> 
  <meta name="report" content="{&quot;pid&quot;: &quot;blog&quot;, &quot;spm&quot;:&quot;1001.2101&quot;}"> 
  <meta name="referrer" content="always"> 
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <link rel="alternate" media="handheld" href="#">  
  <meta name="applicable-device" content="pc"> 
  <link href="https://g.csdnimg.cn/static/logo/favicon32.ico" rel="shortcut icon" type="image/x-icon"> 
  <title>【项目记录】大模型基于llama.cpp在Qemu-riscv64向量扩展指令下的部署_failed to allocate buffer for kv cache-CSDN博客</title>   
  <meta name="keywords" content="failed to allocate buffer for kv cache"> 
  <meta name="csdn-baidu-search" content="{&quot;autorun&quot;:true,&quot;install&quot;:true,&quot;keyword&quot;:&quot;failed to allocate buffer for kv cache&quot;}"> 
  <meta name="description" content="文章浏览阅读1.8k次，点赞9次，收藏10次。大模型基于llama.cpp在Qemu-riscv64向量扩展指令架构的模拟器上部署_failed to allocate buffer for kv cache"> 
  <link rel="stylesheet" type="text/css" href="https://csdnimg.cn/release/blogv2/dist/pc/css/detail_enter-d4fc849858.min.css">  
  <link rel="stylesheet" type="text/css" href="https://csdnimg.cn/release/blogv2/dist/pc/themesSkin/skin-blogstar2024/skin-blogstar2024-fc11876db1.min.css">    
  <meta name="toolbar" content="{&quot;type&quot;:&quot;0&quot;,&quot;fixModel&quot;:&quot;1&quot;}">    
  <link rel="stylesheet" type="text/css" href="https://csdnimg.cn/public/sandalstrap/1.4/css/sandalstrap.min.css"> 
  <style>
        .MathJax, .MathJax_Message, .MathJax_Preview{
            display: none
        }
    </style>    
 	<style>
	main div.blog-content-box pre {
		max-height: 100%;
		overflow-y: hidden;
	}
	</style>
 </head>  
 <body class="nodata  " style=""> 
  <div id="toolbarBox" style="min-height: 48px;"></div>    
  <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/pc/css/blog_code-01256533b5.min.css"> 
  <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/chart-3456820cac.css"> 
  <link rel="stylesheet" href="https://g.csdnimg.cn/lib/swiper/6.0.4/css/swiper.css">   
  <div class="main_father clearfix d-flex justify-content-center mainfather-concision" style="height:100%;"> 
   <div class="container clearfix container-concision" id="mainBox">  
    <main>  
     <div class="blog-content-box"> 
      <div class="article-header-box"> 
       <div class="article-header"> 
        <div class="article-title-box"> 
         <h1 class="title-article" id="articleContentId">【项目记录】大模型基于llama.cpp在Qemu-riscv64向量扩展指令下的部署</h1> 
        </div> 
        <div class="article-info-box"> 
         <div class="article-bar-top"> 
          <img class="article-type-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/original.png" alt=""> 
          <div class="bar-content"> 
           <a href="https://mall.csdn.net/vip" data-report-query="spm=3001.10404" data-report-click="{&quot;spm&quot;:&quot;3001.10404&quot;}" data-report-view="{&quot;spm&quot;:&quot;3001.10404&quot;}" class="article-vip-box" target="_blank"><img class="article-vip-img-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/identityVipNew.png" alt=""></a> 
           <a class="follow-nickName " href="https://kgback.blog.csdn.net" target="_blank" rel="noopener" title="KGback">KGback</a> 
           <img class="article-time-img article-heard-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newUpTime2.png" alt=""> 
           <span class="time">已于&nbsp;2024-11-07 09:59:17&nbsp;修改</span> 
           <div class="read-count-box"> 
            <img class="article-read-img article-heard-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/articleReadEyes2.png" alt=""> 
            <span class="read-count">阅读量1.8k</span> 
            <a id="blog_detail_zk_collection" class="un-collection" data-report-click="{&quot;mod&quot;:&quot;popu_823&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4232&quot;,&quot;ab&quot;:&quot;new&quot;}"> <img class="article-collect-img article-heard-img un-collect-status isdefault" style="display:inline-block" src="https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollect2.png" alt=""> <img class="article-collect-img article-heard-img collect-status isactive" style="display:none" src="https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollectionActive2.png" alt=""> <span class="name">收藏</span> <span class="get-collection"> 10 </span> </a> 
            <div class="read-count-box is-like" data-type="top"> 
             <img class="article-read-img article-heard-img" style="display:none" id="is-like-imgactive-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newHeart2023Active.png" alt=""> 
             <img class="article-read-img article-heard-img" style="display:block" id="is-like-img-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newHeart2023Black.png" alt=""> 
             <span class="read-count" id="blog-digg-num">点赞数 9 </span> 
            </div> 
           </div> 
          </div> 
         </div> 
         <div class="blog-tags-box"> 
          <div class="tags-box artic-tag-box"> 
           <span class="label">分类专栏：</span> 
           <a class="tag-link" href="https://blog.csdn.net/qq_39815222/category_12826738.html" target="_blank" rel="noopener"># Transformer模型</a> 
           <span class="label">文章标签：</span> 
           <a rel="noopener" data-report-query="spm=1001.2101.3001.4223" data-report-click="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;llama&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;llama\&quot;}&quot;}" data-report-view="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;llama&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;llama\&quot;}&quot;}" class="tag-link" href="https://so.csdn.net/so/search/s.do?q=llama&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" target="_blank">llama</a> 
           <a rel="noopener" data-report-query="spm=1001.2101.3001.4223" data-report-click="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;人工智能&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;人工智能\&quot;}&quot;}" data-report-view="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;人工智能&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;人工智能\&quot;}&quot;}" class="tag-link" href="https://so.csdn.net/so/search/s.do?q=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" target="_blank">人工智能</a> 
           <a rel="noopener" data-report-query="spm=1001.2101.3001.4223" data-report-click="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;RISCV&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;RISCV\&quot;}&quot;}" data-report-view="{&quot;mod&quot;:&quot;popu_626&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4223&quot;,&quot;strategy&quot;:&quot;RISCV&quot;,&quot;ab&quot;:&quot;new&quot;,&quot;extra&quot;:&quot;{\&quot;searchword\&quot;:\&quot;RISCV\&quot;}&quot;}" class="tag-link" href="https://so.csdn.net/so/search/s.do?q=RISCV&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" target="_blank">RISCV</a> 
          </div> 
         </div> 
         <div class="up-time">
          <span>于&nbsp;2024-10-03 23:12:55&nbsp;首次发布</span>
         </div> 
         <div class="slide-content-box"> 
          <div class="article-copyright"> 
           <div class="creativecommons">
             版权声明：本文为博主原创文章，遵循
            <a href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener"> CC 4.0 BY-SA </a>版权协议，转载请附上原文出处链接和本声明。 
           </div> 
           <div class="article-source-link">
             本文链接：
            <a href="https://blog.csdn.net/qq_39815222/article/details/142689795" target="_blank">https://blog.csdn.net/qq_39815222/article/details/142689795</a> 
           </div> 
          </div> 
         </div> 
         <div class="operating"> 
          <a class="href-article-edit slide-toggle">版权</a> 
         </div> 
        </div> 
       </div> 
      </div> 
      <div id="blogHuaweiyunAdvert"></div> 
      <article class="baidu_pl"> 
       <div id="article_content" class="article_content clearfix"> 
        <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/kdoc_html_views-1a98987dfd.css"> 
        <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/ck_htmledit_views-704d5b9767.css"> 
        <div id="content_views" class="markdown_views prism-atom-one-dark"> 
         <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
          <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
         </svg> 
         <h2><a id="_0"></a>概述</h2> 
         <p>本文在qemu-riscv64平台上，利用向量扩展指令加速运行基于llama.cpp构建的大模型。<br> 参考博客链接：<br> <a href="https://cloud-v.co/blog/risc-v-1/accelerating-llama-cpp-with-risc-v-vector-extension-3" rel="nofollow">Accelerating llama.cpp with RISC-V Vector Extension</a><br> <a href="https://www.youtube.com/watch?v=1rfohX2peDQ" rel="nofollow">基于RVV的llama.cpp在Banana Pi F3 RISCV开发板上的演示</a></p> 
         <p>源码分析参考链接：<br> <a href="https://blog.csdn.net/qq_39815222/article/details/143578039">KGback：【项目分析】llama.cpp</a></p> 
         <h2><a id="20241002_OKqemukilled_10"></a>2024/10/02: 工具准备OK，但qemu运行时被killed</h2> 
         <h3><a id="_11"></a>工具版本</h3> 
         <p>Qemu：<br> <img src="https://i-blog.csdnimg.cn/direct/02aa459ae2f24ba9a2fa6c759f1847cd.png" alt="在这里插入图片描述"><br> Gcc版本：<br> <a href="https://github.com/riscv-collab/riscv-gnu-toolchain/releases/download/2024.09.03/riscv64-glibc-ubuntu-20.04-llvm-nightly-2024.09.03-nightly.tar.gz">Github Release</a></p> 
         <p>llama.cpp：<br> <a href="https://github.com/ggerganov/llama.cpp.git">llama.cpp Github 10月2号pull</a></p> 
         <p>llama-7b模型版本：<br> <a href="https://huggingface.co/TheBloke/CodeLlama-7B-GGUF/resolve/main/codellama-7b.Q4_K_M.gguf" rel="nofollow">Huggingface gguf文件</a></p> 
         <h3><a id="_24"></a>编译</h3> 
         <h4><a id="llamacpp_26"></a>llama.cpp编译</h4> 
         <pre><code>cd llama.cpp
make   RISCV_CROSS_COMPILE=1 
</code ></pre> 
         <h3><a id="_33"></a>运行命令</h3> 
         <blockquote> 
          <p>qemu-riscv64 -L /home/kevin/data/projects/tools/riscv64_linux_gcc/sysroot -cpu rv64,v=true,vlen=256,elen=64,vext_spec=v1.0 ./llama-server -m /home/kevin/data/projects/kg_proj/rvv_transformer/codellama-7b.Q4_K_M.gguf -p “Anything” -n 9</p> 
         </blockquote> 
         <h3><a id="_36"></a>问题</h3> 
         <h4><a id="_38"></a>命令运行现象</h4> 
         <p><img src="https://i-blog.csdnimg.cn/direct/7765a3a732b146b8b11d92772334846c.png" alt="在这里插入图片描述"><img src="https://i-blog.csdnimg.cn/direct/b27ac4e5a092484197d55950e036be48.png" alt="在这里插入图片描述"><br> <img src="https://i-blog.csdnimg.cn/direct/9e34248213564bf097436b14420b45ad.png" alt="在这里插入图片描述"></p> 
         <h4><a id="_42"></a>可能原因</h4> 
         <p>运行内存可能太小</p> 
         <h2><a id="20241003_10xEtokenizerkilled_46"></a>2024/10/03: 使用10xE团队的最新版，解决tokenizer的问题，但还是被killed</h2> 
         <h3><a id="Github_47"></a>最新版Github链接</h3> 
         <p><a href="https://github.com/Tameem-10xE/llama.cpp.git">Tameem-10xE/llama.cpp Github </a></p> 
         <h3><a id="7Bkilled_50"></a>问题：运行7B模型被killed</h3> 
         <h4><a id="_51"></a>运行现象</h4> 
         <p><img src="https://i-blog.csdnimg.cn/direct/748d531282234fc6a5556ef3ecdd9b6a.png" alt="在这里插入图片描述"><br> <img src="https://i-blog.csdnimg.cn/direct/18cc50015f854e389cc8d11a79e3036b.png" alt="在这里插入图片描述"><br> <img src="https://i-blog.csdnimg.cn/direct/c6e997985d45468eb49ed20926021e9f.png" alt="在这里插入图片描述"></p> 
         <h4><a id="_55"></a>可能原因</h4> 
         <p>有可能跟qemu运行的swapfile有关<br> 可以团队成员提的一个issue：<br> <a href="https://github.com/ggerganov/llama.cpp/issues/2500">Github Issue： qemu-riscv64 unexpectedly reached EOF error</a></p> 
         <h4><a id="_60"></a>解决办法</h4> 
         <p>先尝试换一个更小的模型试试，不行就解决swapfile的问题</p> 
         <h3><a id="3Bmodelfailed_to_allocate_buffer_of_size_63"></a>运行3B规模的model的现象：failed to allocate buffer of size</h3> 
         <pre><code  style="height: 50vh;" class="prism language-bash">kevin@BRICKHOUSE01:~/data/projects/kg_proj/rvv_transformer/llama.cpp$ qemu-riscv64  -L /home/kevin/data/projects/tools/riscv64_linux_gcc/sysroot  -cpu rv64,v<span class="token operator">=</span>true,vlen<span class="token operator">=</span><span class="token number">256</span>,elen<span class="token operator">=</span><span class="token number">64</span>,vext_spec<span class="token operator">=</span>v1.0 ./llama-cli -m /home/kevin/data/projects/kg_proj/rvv_transformer/Llama-3.2-3B-Instruct-IQ3_M.gguf -p <span class="token string">"Anything"</span> -n <span class="token number">9</span>
Log start
main: build <span class="token operator">=</span> <span class="token number">3733</span> <span class="token punctuation">(</span>e5701063<span class="token punctuation">)</span>
main: built with riscv64-unknown-linux-gnu-gcc <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token number">13.2</span>.0 <span class="token keyword">for</span> riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with <span class="token number">35</span> key-value pairs and <span class="token number">255</span> tensors from /home/kevin/data/projects/kg_proj/rvv_transformer/Llama-3.2-3B-Instruct-IQ3_M.gguf <span class="token punctuation">(</span>version GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">))</span>
llama_model_loader: Dumping metadata keys/values. Note: KV overrides <span class="token keyword">do</span> not apply <span class="token keyword">in</span> this output.
llama_model_loader: - kv   <span class="token number">0</span>:                       general.architecture str              <span class="token operator">=</span> llama
llama_model_loader: - kv   <span class="token number">1</span>:                               general.type str              <span class="token operator">=</span> model
llama_model_loader: - kv   <span class="token number">2</span>:                               general.name str              <span class="token operator">=</span> Llama <span class="token number">3.2</span> 3B Instruct
llama_model_loader: - kv   <span class="token number">3</span>:                           general.finetune str              <span class="token operator">=</span> Instruct
llama_model_loader: - kv   <span class="token number">4</span>:                           general.basename str              <span class="token operator">=</span> Llama-3.2
llama_model_loader: - kv   <span class="token number">5</span>:                         general.size_label str              <span class="token operator">=</span> 3B
llama_model_loader: - kv   <span class="token number">6</span>:                            general.license str              <span class="token operator">=</span> llama3.2
llama_model_loader: - kv   <span class="token number">7</span>:                               general.tags arr<span class="token punctuation">[</span>str,6<span class="token punctuation">]</span>       <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"facebook"</span>, <span class="token string">"meta"</span>, <span class="token string">"pytorch"</span>, <span class="token string">"llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["</span>en<span class="token string">", "</span>de<span class="token string">", "</span>fr<span class="token string">", "</span>it<span class="token string">", "</span>pt<span class="token string">", "</span>hi<span class="token string">", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 28
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  19:                          general.file_type u32              = 27
llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = ["</span><span class="token operator">!</span><span class="token string">", "</span><span class="token punctuation">\</span>"<span class="token string">", "</span>#<span class="token string">", "</span>$<span class="token string">", "</span>%<span class="token string">", "</span><span class="token operator">&amp;</span><span class="token string">", "</span>'<span class="token string">", ...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = ["</span>Ġ Ġ<span class="token string">", "</span>Ġ ĠĠĠ<span class="token string">", "</span>ĠĠ ĠĠ<span class="token string">", "</span><span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">27</span>:                tokenizer.ggml.bos_token_id u32              <span class="token operator">=</span> <span class="token number">128000</span>
llama_model_loader: - kv  <span class="token number">28</span>:                tokenizer.ggml.eos_token_id u32              <span class="token operator">=</span> <span class="token number">128009</span>
llama_model_loader: - kv  <span class="token number">29</span>:                    tokenizer.chat_template str              <span class="token operator">=</span> <span class="token punctuation">{
            <!-- --></span><span class="token punctuation">{
            <!-- --></span>- bos_token <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">\</span>n<span class="token punctuation">{
            <!-- --></span>%- <span class="token keyword">if</span> custom_tools <span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">30</span>:               general.quantization_version u32              <span class="token operator">=</span> <span class="token number">2</span>
llama_model_loader: - kv  <span class="token number">31</span>:                      quantize.imatrix.file str              <span class="token operator">=</span> /models_out/Llama-3.2-3B-Instruct-GGU<span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">32</span>:                   quantize.imatrix.dataset str              <span class="token operator">=</span> /training_dir/calibration_datav3.txt
llama_model_loader: - kv  <span class="token number">33</span>:             quantize.imatrix.entries_count i32              <span class="token operator">=</span> <span class="token number">196</span>
llama_model_loader: - kv  <span class="token number">34</span>:              quantize.imatrix.chunks_count i32              <span class="token operator">=</span> <span class="token number">125</span>
llama_model_loader: - <span class="token builtin class-name">type</span>  f32:   <span class="token number">58</span> tensors
llama_model_loader: - <span class="token builtin class-name">type</span> q4_K:   <span class="token number">59</span> tensors
llama_model_loader: - <span class="token builtin class-name">type</span> q6_K:    <span class="token number">1</span> tensors
llama_model_loader: - <span class="token builtin class-name">type</span> iq3_s:  <span class="token number">137</span> tensors
llm_load_vocab: special tokens cache size <span class="token operator">=</span> <span class="token number">256</span>
llm_load_vocab: token to piece cache size <span class="token operator">=</span> <span class="token number">0.7999</span> MB
llm_load_print_meta: <span class="token function">format</span>           <span class="token operator">=</span> GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">)</span>
llm_load_print_meta: arch             <span class="token operator">=</span> llama
llm_load_print_meta: vocab <span class="token builtin class-name">type</span>       <span class="token operator">=</span> BPE
llm_load_print_meta: n_vocab          <span class="token operator">=</span> <span class="token number">128256</span>
llm_load_print_meta: n_merges         <span class="token operator">=</span> <span class="token number">280147</span>
llm_load_print_meta: vocab_only       <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_ctx_train      <span class="token operator">=</span> <span class="token number">131072</span>
llm_load_print_meta: n_embd           <span class="token operator">=</span> <span class="token number">3072</span>
llm_load_print_meta: n_layer          <span class="token operator">=</span> <span class="token number">28</span>
llm_load_print_meta: n_head           <span class="token operator">=</span> <span class="token number">24</span>
llm_load_print_meta: n_head_kv        <span class="token operator">=</span> <span class="token number">8</span>
llm_load_print_meta: n_rot            <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_swa            <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_embd_head_k    <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_embd_head_v    <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_gqa            <span class="token operator">=</span> <span class="token number">3</span>
llm_load_print_meta: n_embd_k_gqa     <span class="token operator">=</span> <span class="token number">1024</span>
llm_load_print_meta: n_embd_v_gqa     <span class="token operator">=</span> <span class="token number">1024</span>
llm_load_print_meta: f_norm_eps       <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_norm_rms_eps   <span class="token operator">=</span> <span class="token number">1</span>.0e-05
llm_load_print_meta: f_clamp_kqv      <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_max_alibi_bias <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_logit_scale    <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: n_ff             <span class="token operator">=</span> <span class="token number">8192</span>
llm_load_print_meta: n_expert         <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_expert_used    <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: causal attn      <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: pooling <span class="token builtin class-name">type</span>     <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: rope <span class="token builtin class-name">type</span>        <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: rope scaling     <span class="token operator">=</span> linear
llm_load_print_meta: freq_base_train  <span class="token operator">=</span> <span class="token number">500000.0</span>
llm_load_print_meta: freq_scale_train <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: n_ctx_orig_yarn  <span class="token operator">=</span> <span class="token number">131072</span>
llm_load_print_meta: rope_finetuned   <span class="token operator">=</span> unknown
llm_load_print_meta: ssm_d_conv       <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_d_inner      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_d_state      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_dt_rank      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_dt_b_c_rms   <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: model <span class="token builtin class-name">type</span>       <span class="token operator">=</span> ?B
llm_load_print_meta: model ftype      <span class="token operator">=</span> IQ3_S mix - <span class="token number">3.66</span> bpw
llm_load_print_meta: model params     <span class="token operator">=</span> <span class="token number">3.21</span> B
llm_load_print_meta: model size       <span class="token operator">=</span> <span class="token number">1.48</span> GiB <span class="token punctuation">(</span><span class="token number">3.96</span> BPW<span class="token punctuation">)</span>
llm_load_print_meta: general.name     <span class="token operator">=</span> Llama <span class="token number">3.2</span> 3B Instruct
llm_load_print_meta: BOS token        <span class="token operator">=</span> <span class="token number">128000</span> <span class="token string">'&lt;|begin_of_text|&gt;'</span>
llm_load_print_meta: EOS token        <span class="token operator">=</span> <span class="token number">128009</span> <span class="token string">'&lt;|eot_id|&gt;'</span>
llm_load_print_meta: LF token         <span class="token operator">=</span> <span class="token number">128</span> <span class="token string">'Ä'</span>
llm_load_print_meta: EOT token        <span class="token operator">=</span> <span class="token number">128009</span> <span class="token string">'&lt;|eot_id|&gt;'</span>
llm_load_print_meta: max token length <span class="token operator">=</span> <span class="token number">256</span>
llm_load_tensors: ggml ctx size <span class="token operator">=</span>    <span class="token number">0.12</span> MiB
llm_load_tensors:        CPU buffer size <span class="token operator">=</span>  <span class="token number">1518.09</span> MiB
<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>.
llama_new_context_with_model: n_ctx      <span class="token operator">=</span> <span class="token number">131072</span>
llama_new_context_with_model: n_batch    <span class="token operator">=</span> <span class="token number">2048</span>
llama_new_context_with_model: n_ubatch   <span class="token operator">=</span> <span class="token number">512</span>
llama_new_context_with_model: flash_attn <span class="token operator">=</span> <span class="token number">0</span>
llama_new_context_with_model: freq_base  <span class="token operator">=</span> <span class="token number">500000.0</span>
llama_new_context_with_model: freq_scale <span class="token operator">=</span> <span class="token number">1</span>
ggml_backend_cpu_buffer_type_alloc_buffer: failed to allocate buffer of size <span class="token number">15032385568</span>
llama_kv_cache_init: failed to allocate buffer <span class="token keyword">for</span> kv cache
llama_new_context_with_model: llama_kv_cache_init<span class="token punctuation">(</span><span class="token punctuation">)</span> failed <span class="token keyword">for</span> self-attention cache
llama_init_from_gpt_params: error: failed to create context with model <span class="token string">'/home/kevin/data/projects/kg_proj/rvv_transformer/Llama-3.2-3B-Instruct-IQ3_M.gguf'</span>
main: error: unable to load model
</code ></pre> 
         <h4><a id="_177"></a>可能原因</h4> 
         <p><a href="https://github.com/ggerganov/llama.cpp/issues/8101">llama.cpp Github issue： Bug: ggml_backend_cpu_buffer_type_alloc_buffer: failed to allocate buffer of size 137438953504</a></p> 
         <h4><a id="_181"></a>解决办法</h4> 
         <p>尝试调整模型的参数：<br> <a href="https://github.com/ggerganov/llama.cpp/tree/master/examples/main">llama.cpp Github参数说明</a></p> 
         <h4><a id="ctx_185"></a>调整ctx参数后成功运行</h4> 
         <pre><code  style="height: 50vh;" class="prism language-shell">kevin@BRICKHOUSE01:~/data/projects/kg_proj/rvv_transformer/llama.cpp$ qemu-riscv64  -L /home/kevin/data/projects/tools/riscv64_linux_gcc/sysroot  -cpu rv64,v<span class="token operator">=</span>true,vlen<span class="token operator">=</span><span class="token number">256</span>,elen<span class="token operator">=</span><span class="token number">64</span>,vext_spec<span class="token operator">=</span>v1.0 ./llama-cli -m /home/kevin/data/projects/kg_proj/rvv_transformer/Llama-3.2-3B-Instruct-IQ3_M.gguf -p <span class="token string">"Anything"</span> -n <span class="token number">9</span> -c <span class="token number">50</span>
Log start
main: build <span class="token operator">=</span> <span class="token number">3733</span> <span class="token punctuation">(</span>e5701063<span class="token punctuation">)</span>
main: built with riscv64-unknown-linux-gnu-gcc <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token number">13.2</span>.0 <span class="token keyword">for</span> riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with <span class="token number">35</span> key-value pairs and <span class="token number">255</span> tensors from /home/kevin/data/projects/kg_proj/rvv_transformer/Llama-3.2-3B-Instruct-IQ3_M.gguf <span class="token punctuation">(</span>version GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">))</span>
llama_model_loader: Dumping metadata keys/values. Note: KV overrides <span class="token keyword">do</span> not apply <span class="token keyword">in</span> this output.
llama_model_loader: - kv   <span class="token number">0</span>:                       general.architecture str              <span class="token operator">=</span> llama
llama_model_loader: - kv   <span class="token number">1</span>:                               general.type str              <span class="token operator">=</span> model
llama_model_loader: - kv   <span class="token number">2</span>:                               general.name str              <span class="token operator">=</span> Llama <span class="token number">3.2</span> 3B Instruct
llama_model_loader: - kv   <span class="token number">3</span>:                           general.finetune str              <span class="token operator">=</span> Instruct
llama_model_loader: - kv   <span class="token number">4</span>:                           general.basename str              <span class="token operator">=</span> Llama-3.2
llama_model_loader: - kv   <span class="token number">5</span>:                         general.size_label str              <span class="token operator">=</span> 3B
llama_model_loader: - kv   <span class="token number">6</span>:                            general.license str              <span class="token operator">=</span> llama3.2
llama_model_loader: - kv   <span class="token number">7</span>:                               general.tags arr<span class="token punctuation">[</span>str,6<span class="token punctuation">]</span>       <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"facebook"</span>, <span class="token string">"meta"</span>, <span class="token string">"pytorch"</span>, <span class="token string">"llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["</span>en<span class="token string">", "</span>de<span class="token string">", "</span>fr<span class="token string">", "</span>it<span class="token string">", "</span>pt<span class="token string">", "</span>hi<span class="token string">", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 28
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  19:                          general.file_type u32              = 27
llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = ["</span><span class="token operator">!</span><span class="token string">", "</span><span class="token punctuation">\</span>"<span class="token string">", "</span>#<span class="token string">", "</span>$<span class="token string">", "</span>%<span class="token string">", "</span><span class="token operator">&amp;</span><span class="token string">", "</span>'<span class="token string">", ...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = ["</span>Ġ Ġ<span class="token string">", "</span>Ġ ĠĠĠ<span class="token string">", "</span>ĠĠ ĠĠ<span class="token string">", "</span><span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">27</span>:                tokenizer.ggml.bos_token_id u32              <span class="token operator">=</span> <span class="token number">128000</span>
llama_model_loader: - kv  <span class="token number">28</span>:                tokenizer.ggml.eos_token_id u32              <span class="token operator">=</span> <span class="token number">128009</span>
llama_model_loader: - kv  <span class="token number">29</span>:                    tokenizer.chat_template str              <span class="token operator">=</span> <span class="token punctuation">{
            <!-- --></span><span class="token punctuation">{
            <!-- --></span>- bos_token <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">\</span>n<span class="token punctuation">{
            <!-- --></span>%- <span class="token keyword">if</span> custom_tools <span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">30</span>:               general.quantization_version u32              <span class="token operator">=</span> <span class="token number">2</span>
llama_model_loader: - kv  <span class="token number">31</span>:                      quantize.imatrix.file str              <span class="token operator">=</span> /models_out/Llama-3.2-3B-Instruct-GGU<span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">32</span>:                   quantize.imatrix.dataset str              <span class="token operator">=</span> /training_dir/calibration_datav3.txt
llama_model_loader: - kv  <span class="token number">33</span>:             quantize.imatrix.entries_count i32              <span class="token operator">=</span> <span class="token number">196</span>
llama_model_loader: - kv  <span class="token number">34</span>:              quantize.imatrix.chunks_count i32              <span class="token operator">=</span> <span class="token number">125</span>
llama_model_loader: - <span class="token builtin class-name">type</span>  f32:   <span class="token number">58</span> tensors
llama_model_loader: - <span class="token builtin class-name">type</span> q4_K:   <span class="token number">59</span> tensors
llama_model_loader: - <span class="token builtin class-name">type</span> q6_K:    <span class="token number">1</span> tensors
llama_model_loader: - <span class="token builtin class-name">type</span> iq3_s:  <span class="token number">137</span> tensors
llm_load_vocab: special tokens cache size <span class="token operator">=</span> <span class="token number">256</span>
llm_load_vocab: token to piece cache size <span class="token operator">=</span> <span class="token number">0.7999</span> MB
llm_load_print_meta: <span class="token function">format</span>           <span class="token operator">=</span> GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">)</span>
llm_load_print_meta: arch             <span class="token operator">=</span> llama
llm_load_print_meta: vocab <span class="token builtin class-name">type</span>       <span class="token operator">=</span> BPE
llm_load_print_meta: n_vocab          <span class="token operator">=</span> <span class="token number">128256</span>
llm_load_print_meta: n_merges         <span class="token operator">=</span> <span class="token number">280147</span>
llm_load_print_meta: vocab_only       <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_ctx_train      <span class="token operator">=</span> <span class="token number">131072</span>
llm_load_print_meta: n_embd           <span class="token operator">=</span> <span class="token number">3072</span>
llm_load_print_meta: n_layer          <span class="token operator">=</span> <span class="token number">28</span>
llm_load_print_meta: n_head           <span class="token operator">=</span> <span class="token number">24</span>
llm_load_print_meta: n_head_kv        <span class="token operator">=</span> <span class="token number">8</span>
llm_load_print_meta: n_rot            <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_swa            <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_embd_head_k    <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_embd_head_v    <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_gqa            <span class="token operator">=</span> <span class="token number">3</span>
llm_load_print_meta: n_embd_k_gqa     <span class="token operator">=</span> <span class="token number">1024</span>
llm_load_print_meta: n_embd_v_gqa     <span class="token operator">=</span> <span class="token number">1024</span>
llm_load_print_meta: f_norm_eps       <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_norm_rms_eps   <span class="token operator">=</span> <span class="token number">1</span>.0e-05
llm_load_print_meta: f_clamp_kqv      <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_max_alibi_bias <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_logit_scale    <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: n_ff             <span class="token operator">=</span> <span class="token number">8192</span>
llm_load_print_meta: n_expert         <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_expert_used    <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: causal attn      <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: pooling <span class="token builtin class-name">type</span>     <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: rope <span class="token builtin class-name">type</span>        <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: rope scaling     <span class="token operator">=</span> linear
llm_load_print_meta: freq_base_train  <span class="token operator">=</span> <span class="token number">500000.0</span>
llm_load_print_meta: freq_scale_train <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: n_ctx_orig_yarn  <span class="token operator">=</span> <span class="token number">131072</span>
llm_load_print_meta: rope_finetuned   <span class="token operator">=</span> unknown
llm_load_print_meta: ssm_d_conv       <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_d_inner      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_d_state      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_dt_rank      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_dt_b_c_rms   <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: model <span class="token builtin class-name">type</span>       <span class="token operator">=</span> ?B
llm_load_print_meta: model ftype      <span class="token operator">=</span> IQ3_S mix - <span class="token number">3.66</span> bpw
llm_load_print_meta: model params     <span class="token operator">=</span> <span class="token number">3.21</span> B
llm_load_print_meta: model size       <span class="token operator">=</span> <span class="token number">1.48</span> GiB <span class="token punctuation">(</span><span class="token number">3.96</span> BPW<span class="token punctuation">)</span>
llm_load_print_meta: general.name     <span class="token operator">=</span> Llama <span class="token number">3.2</span> 3B Instruct
llm_load_print_meta: BOS token        <span class="token operator">=</span> <span class="token number">128000</span> <span class="token string">'&lt;|begin_of_text|&gt;'</span>
llm_load_print_meta: EOS token        <span class="token operator">=</span> <span class="token number">128009</span> <span class="token string">'&lt;|eot_id|&gt;'</span>
llm_load_print_meta: LF token         <span class="token operator">=</span> <span class="token number">128</span> <span class="token string">'Ä'</span>
llm_load_print_meta: EOT token        <span class="token operator">=</span> <span class="token number">128009</span> <span class="token string">'&lt;|eot_id|&gt;'</span>
llm_load_print_meta: max token length <span class="token operator">=</span> <span class="token number">256</span>
llm_load_tensors: ggml ctx size <span class="token operator">=</span>    <span class="token number">0.12</span> MiB
llm_load_tensors:        CPU buffer size <span class="token operator">=</span>  <span class="token number">1518.09</span> MiB
<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>.
llama_new_context_with_model: n_ctx      <span class="token operator">=</span> <span class="token number">64</span>
llama_new_context_with_model: n_batch    <span class="token operator">=</span> <span class="token number">64</span>
llama_new_context_with_model: n_ubatch   <span class="token operator">=</span> <span class="token number">64</span>
llama_new_context_with_model: flash_attn <span class="token operator">=</span> <span class="token number">0</span>
llama_new_context_with_model: freq_base  <span class="token operator">=</span> <span class="token number">500000.0</span>
llama_new_context_with_model: freq_scale <span class="token operator">=</span> <span class="token number">1</span>
llama_kv_cache_init:        CPU KV buffer size <span class="token operator">=</span>     <span class="token number">7.00</span> MiB
llama_new_context_with_model: KV self size  <span class="token operator">=</span>    <span class="token number">7.00</span> MiB, K <span class="token punctuation">(</span>f16<span class="token punctuation">)</span>:    <span class="token number">3.50</span> MiB, V <span class="token punctuation">(</span>f16<span class="token punctuation">)</span>:    <span class="token number">3.50</span> MiB
llama_new_context_with_model:        CPU  output buffer size <span class="token operator">=</span>     <span class="token number">0.49</span> MiB
llama_new_context_with_model:        CPU compute buffer size <span class="token operator">=</span>    <span class="token number">32.06</span> MiB
llama_new_context_with_model: graph nodes  <span class="token operator">=</span> <span class="token number">902</span>
llama_new_context_with_model: graph splits <span class="token operator">=</span> <span class="token number">1</span>

system_info: n_threads <span class="token operator">=</span> <span class="token number">6</span> <span class="token punctuation">(</span>n_threads_batch <span class="token operator">=</span> <span class="token number">6</span><span class="token punctuation">)</span> / <span class="token number">12</span> <span class="token operator">|</span> AVX <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX_VNNI <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX2 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX512 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX512_VBMI <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX512_VNNI <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX512_BF16 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> FMA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> NEON <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> SVE <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> ARM_FMA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> F16C <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> FP16_VA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> RISCV_VECT <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> WASM_SIMD <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> BLAS <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> SSE3 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> SSSE3 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> VSX <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> MATMUL_INT8 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> LLAMAFILE <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span>
sampling seed: <span class="token number">2622092847</span>
sampling params:
        repeat_last_n <span class="token operator">=</span> <span class="token number">64</span>, repeat_penalty <span class="token operator">=</span> <span class="token number">1.000</span>, frequency_penalty <span class="token operator">=</span> <span class="token number">0.000</span>, presence_penalty <span class="token operator">=</span> <span class="token number">0.000</span>
        top_k <span class="token operator">=</span> <span class="token number">40</span>, tfs_z <span class="token operator">=</span> <span class="token number">1.000</span>, top_p <span class="token operator">=</span> <span class="token number">0.950</span>, min_p <span class="token operator">=</span> <span class="token number">0.050</span>, typical_p <span class="token operator">=</span> <span class="token number">1.000</span>, temp <span class="token operator">=</span> <span class="token number">0.800</span>
        mirostat <span class="token operator">=</span> <span class="token number">0</span>, mirostat_lr <span class="token operator">=</span> <span class="token number">0.100</span>, mirostat_ent <span class="token operator">=</span> <span class="token number">5.000</span>
sampler constr:
        logits -<span class="token operator">&gt;</span> logit-bias -<span class="token operator">&gt;</span> penalties -<span class="token operator">&gt;</span> top-k -<span class="token operator">&gt;</span> tail-free -<span class="token operator">&gt;</span> typical -<span class="token operator">&gt;</span> top-p -<span class="token operator">&gt;</span> min-p -<span class="token operator">&gt;</span> temp-ext -<span class="token operator">&gt;</span> softmax -<span class="token operator">&gt;</span> dist
generate: n_ctx <span class="token operator">=</span> <span class="token number">64</span>, n_batch <span class="token operator">=</span> <span class="token number">2048</span>, n_predict <span class="token operator">=</span> <span class="token number">9</span>, n_keep <span class="token operator">=</span> <span class="token number">1</span>


Anything else?
Yes. You can also consider the
llama_perf_print:    sampling <span class="token function">time</span> <span class="token operator">=</span>      <span class="token number">10.96</span> ms /    <span class="token number">11</span> runs   <span class="token punctuation">(</span>    <span class="token number">1.00</span> ms per token,  <span class="token number">1003.37</span> tokens per second<span class="token punctuation">)</span>
llama_perf_print:        load <span class="token function">time</span> <span class="token operator">=</span>   <span class="token number">73770.30</span> ms
llama_perf_print: prompt <span class="token builtin class-name">eval</span> <span class="token function">time</span> <span class="token operator">=</span>   <span class="token number">15435.63</span> ms /     <span class="token number">2</span> tokens <span class="token punctuation">(</span> <span class="token number">7717.81</span> ms per token,     <span class="token number">0.13</span> tokens per second<span class="token punctuation">)</span>
llama_perf_print:        <span class="token builtin class-name">eval</span> <span class="token function">time</span> <span class="token operator">=</span>   <span class="token number">74267.24</span> ms /     <span class="token number">8</span> runs   <span class="token punctuation">(</span> <span class="token number">9283.41</span> ms per token,     <span class="token number">0.11</span> tokens per second<span class="token punctuation">)</span>
llama_perf_print:       total <span class="token function">time</span> <span class="token operator">=</span>   <span class="token number">89764.85</span> ms /    <span class="token number">10</span> tokens
Log end
</code ></pre> 
         <h2><a id="20241005riscvllamacpp_321"></a>2024/10/05：生成非向量支持的riscv版本llama.cpp，进行对比实验</h2> 
         <h3><a id="llamacpp_323"></a>llama.cpp编译</h3> 
         <pre><code class="prism language-shell"><span class="token function">make</span> <span class="token assign-left variable">CC</span><span class="token operator">=</span><span class="token string">"riscv64-unknown-linux-gnu-gcc -march=rv64gc -mabi=lp64d"</span> <span class="token assign-left variable">CXX</span><span class="token operator">=</span><span class="token string">"riscv64-unknown-linux-gnu-g++ -march=rv64gc -mabi=lp64d"</span>
<span class="token function">make</span> <span class="token assign-left variable">CC</span><span class="token operator">=</span><span class="token string">"riscv64-unknown-linux-gnu-gcc -march=rv64gc -mabi=lp64d"</span> <span class="token assign-left variable">CXX</span><span class="token operator">=</span><span class="token string">"riscv64-unknown-linux-gnu-g++ -march=rv64gc -mabi=lp64d"</span>
</code ></pre> 
         <h4><a id="riscv64unknownlinuxgnug_error_marchnative_ISA_string_must_begin_with_rv32_or_rv64_329"></a>报错：riscv64-unknown-linux-gnu-g++: error: ‘-march=native’: ISA string must begin with rv32 or rv64</h4> 
         <p>解决方案：<br> <a href="https://github.com/riscv-collab/riscv-gnu-toolchain/issues/1505">GCC GitHub：riscv64-unknown-linux-gnu-g++: error: ‘-march=native’: ISA string must begin with rv32 or rv64</a></p> 
         <p>即将llama.cpp工程中的Makefile中523和524行修改为rv64gc编译，该部分原先是交叉编译时使能RVV编译，如图所示：</p> 
         <p><img src="https://i-blog.csdnimg.cn/direct/02b791b2a1bc4ef3ac0eeebdc47f054b.png" alt="在这里插入图片描述"></p> 
         <h4><a id="_338"></a>修正后编译命令</h4> 
         <pre><code>make RISCV_CROSS_COMPILE=1
</code ></pre> 
         <h3><a id="_343"></a>运行结果</h3> 
         <pre><code  style="height: 50vh;" class="prism language-shell">kevin@BRICKHOUSE01:~/data/projects/kg_proj/rvv_transformer/llama.cpp.rv64gc$ qemu-riscv64  -L /home/kevin/data/projects/tools/riscv64_linux_gcc/sysroot  -cpu rv64 ./llama-cli -m /home/kevin/data/projects/kg_proj/rvv_transformer/codellama-7b.Q4_K_M.gguf -p <span class="token string">"Anything"</span> -n <span class="token number">100</span> -c <span class="token number">1024</span>
Log start
main: build <span class="token operator">=</span> <span class="token number">3733</span> <span class="token punctuation">(</span>e5701063<span class="token punctuation">)</span>
main: built with riscv64-unknown-linux-gnu-gcc <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token number">13.2</span>.0 <span class="token keyword">for</span> riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with <span class="token number">20</span> key-value pairs and <span class="token number">291</span> tensors from /home/kevin/data/projects/kg_proj/rvv_transformer/codellama-7b.Q4_K_M.gguf <span class="token punctuation">(</span>version GGUF V2<span class="token punctuation">)</span>
llama_model_loader: Dumping metadata keys/values. Note: KV overrides <span class="token keyword">do</span> not apply <span class="token keyword">in</span> this output.
llama_model_loader: - kv   <span class="token number">0</span>:                       general.architecture str              <span class="token operator">=</span> llama
llama_model_loader: - kv   <span class="token number">1</span>:                               general.name str              <span class="token operator">=</span> codellama_codellama-7b-hf
llama_model_loader: - kv   <span class="token number">2</span>:                       llama.context_length u32              <span class="token operator">=</span> <span class="token number">16384</span>
llama_model_loader: - kv   <span class="token number">3</span>:                     llama.embedding_length u32              <span class="token operator">=</span> <span class="token number">4096</span>
llama_model_loader: - kv   <span class="token number">4</span>:                          llama.block_count u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader: - kv   <span class="token number">5</span>:                  llama.feed_forward_length u32              <span class="token operator">=</span> <span class="token number">11008</span>
llama_model_loader: - kv   <span class="token number">6</span>:                 llama.rope.dimension_count u32              <span class="token operator">=</span> <span class="token number">128</span>
llama_model_loader: - kv   <span class="token number">7</span>:                 llama.attention.head_count u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader: - kv   <span class="token number">8</span>:              llama.attention.head_count_kv u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader: - kv   <span class="token number">9</span>:     llama.attention.layer_norm_rms_epsilon f32              <span class="token operator">=</span> <span class="token number">0.000010</span>
llama_model_loader: - kv  <span class="token number">10</span>:                       llama.rope.freq_base f32              <span class="token operator">=</span> <span class="token number">1000000.000000</span>
llama_model_loader: - kv  <span class="token number">11</span>:                          general.file_type u32              <span class="token operator">=</span> <span class="token number">15</span>
llama_model_loader: - kv  <span class="token number">12</span>:                       tokenizer.ggml.model str              <span class="token operator">=</span> llama
llama_model_loader: - kv  <span class="token number">13</span>:                      tokenizer.ggml.tokens arr<span class="token punctuation">[</span>str,32016<span class="token punctuation">]</span>   <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"&lt;unk&gt;"</span>, <span class="token string">"&lt;s&gt;"</span>, <span class="token string">"&lt;/s&gt;"</span>, <span class="token string">"&lt;0x00&gt;"</span>, "<span class="token operator">&lt;</span><span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">14</span>:                      tokenizer.ggml.scores arr<span class="token punctuation">[</span>f32,32016<span class="token punctuation">]</span>   <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.000000</span>, <span class="token number">0.000000</span>, <span class="token number">0.000000</span>, <span class="token number">0.0000</span><span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">15</span>:                  tokenizer.ggml.token_type arr<span class="token punctuation">[</span>i32,32016<span class="token punctuation">]</span>   <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">3</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">16</span>:                tokenizer.ggml.bos_token_id u32              <span class="token operator">=</span> <span class="token number">1</span>
llama_model_loader: - kv  <span class="token number">17</span>:                tokenizer.ggml.eos_token_id u32              <span class="token operator">=</span> <span class="token number">2</span>
llama_model_loader: - kv  <span class="token number">18</span>:            tokenizer.ggml.unknown_token_id u32              <span class="token operator">=</span> <span class="token number">0</span>
llama_model_loader: - kv  <span class="token number">19</span>:               general.quantization_version u32              <span class="token operator">=</span> <span class="token number">2</span>
llama_model_loader: - <span class="token builtin class-name">type</span>  f32:   <span class="token number">65</span> tensors
llama_model_loader: - <span class="token builtin class-name">type</span> q4_K:  <span class="token number">193</span> tensors
llama_model_loader: - <span class="token builtin class-name">type</span> q6_K:   <span class="token number">33</span> tensors
llm_load_vocab: special tokens cache size <span class="token operator">=</span> <span class="token number">3</span>
llm_load_vocab: token to piece cache size <span class="token operator">=</span> <span class="token number">0.1686</span> MB
llm_load_print_meta: <span class="token function">format</span>           <span class="token operator">=</span> GGUF V2
llm_load_print_meta: arch             <span class="token operator">=</span> llama
llm_load_print_meta: vocab <span class="token builtin class-name">type</span>       <span class="token operator">=</span> SPM
llm_load_print_meta: n_vocab          <span class="token operator">=</span> <span class="token number">32016</span>
llm_load_print_meta: n_merges         <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: vocab_only       <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_ctx_train      <span class="token operator">=</span> <span class="token number">16384</span>
llm_load_print_meta: n_embd           <span class="token operator">=</span> <span class="token number">4096</span>
llm_load_print_meta: n_layer          <span class="token operator">=</span> <span class="token number">32</span>
llm_load_print_meta: n_head           <span class="token operator">=</span> <span class="token number">32</span>
llm_load_print_meta: n_head_kv        <span class="token operator">=</span> <span class="token number">32</span>
llm_load_print_meta: n_rot            <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_swa            <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_embd_head_k    <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_embd_head_v    <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_gqa            <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: n_embd_k_gqa     <span class="token operator">=</span> <span class="token number">4096</span>
llm_load_print_meta: n_embd_v_gqa     <span class="token operator">=</span> <span class="token number">4096</span>
llm_load_print_meta: f_norm_eps       <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_norm_rms_eps   <span class="token operator">=</span> <span class="token number">1</span>.0e-05
llm_load_print_meta: f_clamp_kqv      <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_max_alibi_bias <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_logit_scale    <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: n_ff             <span class="token operator">=</span> <span class="token number">11008</span>
llm_load_print_meta: n_expert         <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_expert_used    <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: causal attn      <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: pooling <span class="token builtin class-name">type</span>     <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: rope <span class="token builtin class-name">type</span>        <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: rope scaling     <span class="token operator">=</span> linear
llm_load_print_meta: freq_base_train  <span class="token operator">=</span> <span class="token number">1000000.0</span>
llm_load_print_meta: freq_scale_train <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: n_ctx_orig_yarn  <span class="token operator">=</span> <span class="token number">16384</span>
llm_load_print_meta: rope_finetuned   <span class="token operator">=</span> unknown
llm_load_print_meta: ssm_d_conv       <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_d_inner      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_d_state      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_dt_rank      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_dt_b_c_rms   <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: model <span class="token builtin class-name">type</span>       <span class="token operator">=</span> 7B
llm_load_print_meta: model ftype      <span class="token operator">=</span> Q4_K - Medium
llm_load_print_meta: model params     <span class="token operator">=</span> <span class="token number">6.74</span> B
llm_load_print_meta: model size       <span class="token operator">=</span> <span class="token number">3.80</span> GiB <span class="token punctuation">(</span><span class="token number">4.84</span> BPW<span class="token punctuation">)</span>
llm_load_print_meta: general.name     <span class="token operator">=</span> codellama_codellama-7b-hf
llm_load_print_meta: BOS token        <span class="token operator">=</span> <span class="token number">1</span> <span class="token string">'&lt;s&gt;'</span>
llm_load_print_meta: EOS token        <span class="token operator">=</span> <span class="token number">2</span> <span class="token string">'&lt;/s&gt;'</span>
llm_load_print_meta: UNK token        <span class="token operator">=</span> <span class="token number">0</span> <span class="token string">'&lt;unk&gt;'</span>
llm_load_print_meta: LF token         <span class="token operator">=</span> <span class="token number">13</span> <span class="token string">'&lt;0x0A&gt;'</span>
llm_load_print_meta: PRE token        <span class="token operator">=</span> <span class="token number">32007</span> <span class="token string">'▁&lt;PRE&gt;'</span>
llm_load_print_meta: SUF token        <span class="token operator">=</span> <span class="token number">32008</span> <span class="token string">'▁&lt;SUF&gt;'</span>
llm_load_print_meta: MID token        <span class="token operator">=</span> <span class="token number">32009</span> <span class="token string">'▁&lt;MID&gt;'</span>
llm_load_print_meta: EOT token        <span class="token operator">=</span> <span class="token number">32010</span> <span class="token string">'▁&lt;EOT&gt;'</span>
llm_load_print_meta: max token length <span class="token operator">=</span> <span class="token number">48</span>
llm_load_tensors: ggml ctx size <span class="token operator">=</span>    <span class="token number">0.14</span> MiB
llm_load_tensors:        CPU buffer size <span class="token operator">=</span>  <span class="token number">3891.33</span> MiB
<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>
llama_new_context_with_model: n_ctx      <span class="token operator">=</span> <span class="token number">1024</span>
llama_new_context_with_model: n_batch    <span class="token operator">=</span> <span class="token number">1024</span>
llama_new_context_with_model: n_ubatch   <span class="token operator">=</span> <span class="token number">512</span>
llama_new_context_with_model: flash_attn <span class="token operator">=</span> <span class="token number">0</span>
llama_new_context_with_model: freq_base  <span class="token operator">=</span> <span class="token number">1000000.0</span>
llama_new_context_with_model: freq_scale <span class="token operator">=</span> <span class="token number">1</span>
llama_kv_cache_init:        CPU KV buffer size <span class="token operator">=</span>   <span class="token number">512.00</span> MiB
llama_new_context_with_model: KV self size  <span class="token operator">=</span>  <span class="token number">512.00</span> MiB, K <span class="token punctuation">(</span>f16<span class="token punctuation">)</span>:  <span class="token number">256.00</span> MiB, V <span class="token punctuation">(</span>f16<span class="token punctuation">)</span>:  <span class="token number">256.00</span> MiB
llama_new_context_with_model:        CPU  output buffer size <span class="token operator">=</span>     <span class="token number">0.12</span> MiB
llama_new_context_with_model:        CPU compute buffer size <span class="token operator">=</span>    <span class="token number">98.01</span> MiB
llama_new_context_with_model: graph nodes  <span class="token operator">=</span> <span class="token number">1030</span>
llama_new_context_with_model: graph splits <span class="token operator">=</span> <span class="token number">1</span>

system_info: n_threads <span class="token operator">=</span> <span class="token number">6</span> <span class="token punctuation">(</span>n_threads_batch <span class="token operator">=</span> <span class="token number">6</span><span class="token punctuation">)</span> / <span class="token number">12</span> <span class="token operator">|</span> AVX <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX_VNNI <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX2 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX512 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX512_VBMI <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX512_VNNI <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX512_BF16 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> FMA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> NEON <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> SVE <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> ARM_FMA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> F16C <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> FP16_VA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> RISCV_VECT <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> WASM_SIMD <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> BLAS <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> SSE3 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> SSSE3 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> VSX <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> MATMUL_INT8 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> LLAMAFILE <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span>
sampling seed: <span class="token number">3630221793</span>
sampling params:
        repeat_last_n <span class="token operator">=</span> <span class="token number">64</span>, repeat_penalty <span class="token operator">=</span> <span class="token number">1.000</span>, frequency_penalty <span class="token operator">=</span> <span class="token number">0.000</span>, presence_penalty <span class="token operator">=</span> <span class="token number">0.000</span>
        top_k <span class="token operator">=</span> <span class="token number">40</span>, tfs_z <span class="token operator">=</span> <span class="token number">1.000</span>, top_p <span class="token operator">=</span> <span class="token number">0.950</span>, min_p <span class="token operator">=</span> <span class="token number">0.050</span>, typical_p <span class="token operator">=</span> <span class="token number">1.000</span>, temp <span class="token operator">=</span> <span class="token number">0.800</span>
        mirostat <span class="token operator">=</span> <span class="token number">0</span>, mirostat_lr <span class="token operator">=</span> <span class="token number">0.100</span>, mirostat_ent <span class="token operator">=</span> <span class="token number">5.000</span>
sampler constr:
        logits -<span class="token operator">&gt;</span> logit-bias -<span class="token operator">&gt;</span> penalties -<span class="token operator">&gt;</span> top-k -<span class="token operator">&gt;</span> tail-free -<span class="token operator">&gt;</span> typical -<span class="token operator">&gt;</span> top-p -<span class="token operator">&gt;</span> min-p -<span class="token operator">&gt;</span> temp-ext -<span class="token operator">&gt;</span> softmax -<span class="token operator">&gt;</span> dist
generate: n_ctx <span class="token operator">=</span> <span class="token number">1024</span>, n_batch <span class="token operator">=</span> <span class="token number">2048</span>, n_predict <span class="token operator">=</span> <span class="token number">100</span>, n_keep <span class="token operator">=</span> <span class="token number">1</span>


 Anything is possible <span class="token keyword">if</span> you believe it. The only thing you are not capable of doing is believing <span class="token keyword">in</span> anything.
- John L. Sullivan



- To be honest, I<span class="token string">'ve never had a problem with the fact that I'</span>m a bad person and I<span class="token string">'m a horrible human. I'</span>ve always been okay with that.
- I think it's a mistake to think of yourself as being <span class="token keyword">in</span> the middle of the world, instead of at the
llama_perf_print:    sampling <span class="token function">time</span> <span class="token operator">=</span>      <span class="token number">94.53</span> ms /   <span class="token number">104</span> runs   <span class="token punctuation">(</span>    <span class="token number">0.91</span> ms per token,  <span class="token number">1100.12</span> tokens per second<span class="token punctuation">)</span>
llama_perf_print:        load <span class="token function">time</span> <span class="token operator">=</span>   <span class="token number">44296.57</span> ms
llama_perf_print: prompt <span class="token builtin class-name">eval</span> <span class="token function">time</span> <span class="token operator">=</span>   <span class="token number">79786.77</span> ms /     <span class="token number">4</span> tokens <span class="token punctuation">(</span><span class="token number">19946.69</span> ms per token,     <span class="token number">0.05</span> tokens per second<span class="token punctuation">)</span>
llama_perf_print:        <span class="token builtin class-name">eval</span> <span class="token function">time</span> <span class="token operator">=</span> <span class="token number">2360379.06</span> ms /    <span class="token number">99</span> runs   <span class="token punctuation">(</span><span class="token number">23842.21</span> ms per token,     <span class="token number">0.04</span> tokens per second<span class="token punctuation">)</span>
llama_perf_print:       total <span class="token function">time</span> <span class="token operator">=</span> <span class="token number">2440911.31</span> ms /   <span class="token number">103</span> tokens
Log end
</code ></pre> 
         <h2><a id="_472"></a>参考文献</h2> 
        </div> 
        <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-a5d25dd831.css" rel="stylesheet"> 
        <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-e504d6a974.css" rel="stylesheet"> 
       </div> 
      </article>  
     </div> 
     <div class="directory-boxshadow-dialog" style="display:none;"> 
      <div class="directory-boxshadow-dialog-box"> 
      </div> 
      <div class="vip-limited-time-offer-box-new" id="vip-limited-time-offer-box-new"> 
       <img class="limited-img limited-img-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/vip-limited-close-newWhite.png"> 
       <div class="vip-limited-time-top">
         确定要放弃本次机会？ 
       </div> 
       <span class="vip-limited-time-text">福利倒计时</span> 
       <div class="limited-time-box-new"> 
        <span class="time-hour"></span> 
        <i>:</i> 
        <span class="time-minite"></span> 
        <i>:</i> 
        <span class="time-second"></span> 
       </div> 
       <div class="limited-time-vip-box"> 
        <p> <img class="coupon-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/vip-limited-close-roup.png"> <span class="def">立减 ¥</span> <span class="active limited-num"></span> </p> 
        <span class="">普通VIP年卡可用</span> 
       </div> 
       <a class="limited-time-btn-new" href="https://mall.csdn.net/vip" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.9621&quot;}" data-report-query="spm=1001.2101.3001.9621">立即使用</a> 
      </div> 
     </div> 
     <div class="more-toolbox-new more-toolbar" id="toolBarBox"> 
      <div class="left-toolbox"> 
       <div class="toolbox-left"> 
        <div class="profile-box"> 
         <a class="profile-href" target="_blank" href="https://kgback.blog.csdn.net"><img class="profile-img" src="https://profile-avatar.csdnimg.cn/f2f090a2593e4cbb8cf3c3a9aa4b9457_qq_39815222.jpg!1"> <span class="profile-name"> KGback </span> </a> 
        </div> 
        <div class="profile-attend"> 
         <a class="tool-attend tool-bt-button tool-bt-attend" href="javascript:;" data-report-view="{&quot;mod&quot;:&quot;1592215036_002&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4232&quot;,&quot;extend1&quot;:&quot;已关注&quot;}">已关注</a> 
         <a class="tool-item-follow active-animation" style="display:none;">已关注</a> 
        </div> 
       </div> 
       <div class="toolbox-middle"> 
        <ul class="toolbox-list"> 
         <li class="tool-item tool-item-size tool-active is-like" id="is-like" data-type="bottom"> <a class="tool-item-href"> <img style="display:none;" id="is-like-imgactive-animation-like" class="animation-dom active-animation" src="https://csdnimg.cn/release/blogv2/dist/pc/img/tobarThumbUpactive.png" alt=""> <img class="isactive" style="display:none" id="is-like-imgactive" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/like-active.png" alt=""> <img class="isdefault" style="display:block" id="is-like-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/like.png" alt=""> <span id="spanCount" class="count "> 9 </span> </a> 
          <div class="tool-hover-tip">
           <span class="text space">点赞</span>
          </div> </li> 
         <li class="tool-item tool-item-size tool-active is-unlike" id="is-unlike"> <a class="tool-item-href"> <img class="isactive" style="margin-right:0px;display:none" id="is-unlike-imgactive" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/unlike-active.png" alt=""> <img class="isdefault" style="margin-right:0px;display:block" id="is-unlike-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/unlike.png" alt=""> <span id="unlikeCount" class="count "></span> </a> 
          <div class="tool-hover-tip">
           <span class="text space">踩</span>
          </div> </li> 
         <li class="tool-item tool-item-size tool-active is-collection "> <a class="tool-item-href" href="javascript:;" data-report-click="{&quot;mod&quot;:&quot;popu_824&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4130&quot;,&quot;ab&quot;:&quot;new&quot;}"> <img style="display:none" id="is-collection-img-collection" class="animation-dom active-animation" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/collect-active.png" alt=""> <img class="isdefault" id="is-collection-img" style="display:block" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/collect.png" alt=""> <img class="isactive" id="is-collection-imgactive" style="display:none" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newCollectActive.png" alt=""> <span class="count get-collection " data-num="10" id="get-collection"> 10 </span> </a> 
          <div class="tool-hover-tip collect"> 
           <div class="collect-operate-box"> 
            <span class="collect-text" id="is-collection"> 收藏 </span> 
           </div> 
          </div> 
          <div class="tool-active-list"> 
           <div class="text">
             觉得还不错? 
            <span class="collect-text" id="tool-active-list-collection"> 一键收藏 </span> 
            <img id="tool-active-list-close" src="https://csdnimg.cn/release/blogv2/dist/pc/img/collectionCloseWhite.png" alt=""> 
           </div> 
          </div> </li> 
         <li class="tool-item tool-item-size tool-active tool-item-comment"> 
          <div class="guide-rr-first"> 
           <img src="https://csdnimg.cn/release/blogv2/dist/pc/img/guideRedReward01.png" alt=""> 
           <button class="btn-guide-known">知道了</button> 
          </div> <a class="tool-item-href go-side-comment" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7009&quot;}"> <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/comment.png" alt=""> <span class="count"> 0 </span> </a> 
          <div class="tool-hover-tip">
           <span class="text space">评论</span>
          </div> </li> 
         <li class="tool-item tool-item-size tool-active tool-QRcode" data-type="article" id="tool-share"> <a class="tool-item-href" href="javascript:;" data-report-view="{&quot;spm&quot;:&quot;3001.4129&quot;,&quot;extra&quot;:{&quot;type&quot;:&quot;blogdetail&quot;}}"> <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/share.png" alt=""> <span class="count">分享</span> </a> 
          <div class="QRcode active" id="tool-QRcode"> 
           <div class="share-bg-box"> 
            <div class="share-content"> 
             <a id="copyPosterUrl" data-type="link" class="btn-share">复制链接</a> 
            </div> 
            <div class="share-content"> 
             <a class="btn-share" data-type="qq">分享到 QQ</a> 
            </div> 
            <div class="share-content"> 
             <a class="btn-share" data-type="weibo">分享到新浪微博</a> 
            </div> 
            <div class="share-code"> 
             <div class="share-code-box" id="shareCode"></div> 
             <div class="share-code-text"> 
              <img src="https://csdnimg.cn/release/blogv2/dist/pc/img/share/icon-wechat.png" alt="">扫一扫 
             </div> 
            </div> 
           </div> 
          </div> </li> 
         <li class="tool-item tool-item-size tool-active tool-item-reward"> <a class="tool-item-href" href="javascript:;" data-report-click="{&quot;mod&quot;:&quot;popu_830&quot;,&quot;spm&quot;:&quot;1001.2101.3001.4237&quot;,&quot;dest&quot;:&quot;&quot;,&quot;ab&quot;:&quot;new&quot;}"> <img class="isdefault reward-bt" id="rewardBtNew" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/reward.png" alt="打赏"> <span class="count">打赏</span> </a> 
          <div class="tool-hover-tip">
           <span class="text space">打赏</span>
          </div> </li> 
         <li class="tool-item tool-item-size tool-active is-more" id="is-more"> <a class="tool-item-href"> <img class="isdefault" style="margin-right:0px;display:block" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/more.png" alt=""> <span class="count"></span> </a> 
          <div class="more-opt-box"> 
           <div class="mini-box"> 
            <a class="tool-item-href" id="rewardBtNewHide" data-report-click="{&quot;spm&quot;:&quot;3001.4237&quot;,&quot;extra&quot;:&quot;{\&quot;type\&quot;:\&quot;hide\&quot;}&quot;}"> <img class="isdefault reward-bt" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/reward.png" alt="打赏"> <span class="count">打赏</span> </a> 
            <a class="tool-item-href" id="toolReportBtnHide"> <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/report.png" alt=""> <span class="count">举报</span> </a> 
           </div> 
           <div class="normal-box"> 
            <a class="tool-item-href" id="toolReportBtnHideNormal"> <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/report.png" alt=""> <span class="count">举报</span> </a> 
           </div> 
          </div> </li> 
        </ul> 
       </div> 
       <div class="toolbox-right"> 
        <div class="tool-directory"> 
         <a class="bt-columnlist-show" data-id="12826738" data-free="true" data-description="关于Transformer模型的学习记录、项目记录" data-subscribe="false" data-title="Transformer模型" data-img="https://i-blog.csdnimg.cn/columns/default/20201014180756918.png?x-oss-process=image/resize,m_fixed,h_224,w_224" data-url="https://blog.csdn.net/qq_39815222/category_12826738.html" data-sum="5" data-people="0" data-price="0" data-hotrank="0" data-status="true" data-oldprice="0" data-join="false" data-studyvip="true" data-studysubscribe="false" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.6334&quot;,&quot;extend1&quot;:&quot;专栏目录&quot;}" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.6334&quot;,&quot;extend1&quot;:&quot;专栏目录&quot;}">专栏目录</a> 
        </div> 
       </div> 
      </div> 
     </div>    
     <a id="commentBox" name="commentBox"></a> 
    </main> 
   </div> 
   <div class="recommend-right1  align-items-stretch clearfix" id="rightAsideConcision" data-type="recommend"> 
    <aside class="recommend-right_aside"> 
     <div id="recommend-right-concision"> 
      <div class="flex-column aside-box groupfile" id="groupfileConcision"> 
       <div class="groupfile-div1"> 
        <h3 class="aside-title">目录</h3> 
        <div class="align-items-stretch group_item"> 
         <div class="pos-box"> 
          <div class="scroll-box"> 
           <div class="toc-box"></div> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
     </div> 
    </aside> 
   </div> 
  </div> 
  <div class="mask-dark"></div> 
  <div class="skin-boxshadow"></div> 
  <div class="directory-boxshadow"></div> 
  <div style="display:none;"> 
   <img src="" onerror="setTimeout(function(){if(!/(csdn.net|iteye.com|baiducontent.com|googleusercontent.com|360webcache.com|sogoucdn.com|bingj.com|baidu.com)$/.test(window.location.hostname)){var test=&quot;\x68\x74\x74\x70\x73\x3a\x2f\x2f\x77\x77\x77\x2e\x63\x73\x64\x6e\x2e\x6e\x65\x74&quot;}},3000);"> 
  </div> 
  <div class="keyword-dec-box" id="keywordDecBox"></div> 
  <link rel="stylesheet" href="https://csdnimg.cn/release/blog_editor_html/release1.6.12/ckeditor/plugins/chart/chart.css">        
  <link rel="stylesheet" href="https://g.csdnimg.cn/lib/cboxEditor/1.1.6/embed-editor.min.css"> 
  <link rel="stylesheet" href="https://csdnimg.cn/release/blog_editor_html/release1.6.12/ckeditor/plugins/codesnippet/lib/highlight/styles/atom-one-dark.css">                  
 </body>
</html>