---
layout: post
title: 大模型知识点
date: 2025-02-28 20:44 +0800
categories: [读书笔记, paper]
tags: []
math: true
img_path: /assets/img/paper/
---


## 一、大模型的基础知识

### 1.1 add和norm
[Transformer中Add&Norm层的理解](https://blog.csdn.net/weixin_51756104/article/details/127232344)

<font color="#e5b9b7">Add操作</font>：残差连接就是把网络的输入和输出相加，即网络的输出为F(x)+x，在网络结构比较深的时候，网络梯度反向传播更新参数时，容易造成梯度消失的问题，但是如果每层的输出都加上一个x的时候，就变成了F(x)+x，对x求导结果为1，有效解决了梯度消失问题。


Norm操作：输入的词向量的形状是（x，y，z），x对应batch，y对应句子长度，z对应 词向量维度，Norm分有BN，LN，IN，具体可以看上方博客

### 1.2 梯度导致的数据维度变化
![overall]({{ page.img_path }}overall.png){: width="972" height="200" }

![matric4]({{ page.img_path }}matric4.png){: width="972" height="200" }

- y是标量，x是向量
![matric1]({{ page.img_path }}matric1.png){: width="972" height="200" }

- y是向量，x是标量
![matric2]({{ page.img_path }}matric2.png){: width="972" height="200" }

- 内积求导（注意：向量对向量求导结果是一个矩阵）
![neiji]({{ page.img_path }}neiji.png){: width="972" height="200" }
![matric3]({{ page.img_path }}matric3.png){: width="972" height="200" }

- 样例(加粗代表向量)

$**y**=**x^TA**$，求导为$**A^T**$

正向传播和反向传播数据依赖：因为需要存储正向的所有中间结果

![direction]({{ page.img_path }}direction.png){: width="972" height="200" }

### 1.3 回归和分类的区别

回归是一个单连续值的输出，和真实值的区别作为loss

分类是多个输出，代表预测为第i类的置信度




### 1.4 MLA

通过潜在的向量压缩KV Cache


### 1.5 RoPE

位置编码和相对位置有关，绝对位置不是那么重要 。RoPE位置编码的要点：

1. 先做多Head的投影，再加上旋转位置编码
2. 用的仍然是正弦和余弦函数操作，当两个向量计算内积时，直接转换成了包含相对位置信息。
3. 只对Q，K做旋转位置编码，V不变
4. d为embedding的维度，m为绝对位置

![RoPE]({{ page.img_path }}RoPE.png){: width="972" height="200" }

### 1.6 MoE

下面是计算路由的函数，sigmoid

![MoE]({{ page.img_path }}MoE.png){: width="972" height="200" }
![less_loss]({{ page.img_path }}less_loss.png){: width="972" height="200" }



## 二、Switch Transformers

![route_moe]({{ page.img_path }}route_moe.png){: width="972" height="589" }

“专家权重”或“可信度”通常由一个 **路由器 (Router)** 或 **门控网络 (Gating Network)** 来计算:

每个 token先经过前面的自注意力层与 Add+Norm 等操作后，得到一段向量h。路由器常常只是一个 **单层线性变换**（有时会加上激活函数），把 h 投影到“专家数”维度上。获得向量z，z 的每个分量对应一个专家（Expert）在“未归一化”下的得分，将 z 做 softmax 得到概率分布$p_i$，对应第i个专家的可信度。

![split]({{ page.img_path }}split.png){: width="972" height="589" }

在多核（或多GPU/TPU）环境中对大型模型进行并行化的split方案：

**第一行**：**模型权重在各核心之间的切分方式**、**第二行**：**输入数据在各核心之间的切分方式**。**同一个颜色 / 同一个大矩形**往往表示的是同一个权重矩阵的某一块；

- **Data Parallelism**：每个设备（核心）都持有**完整的模型参数**，但只处理一部分的数据（即 batch 切分）
- **Model and Data Parallelism**：不同的核心各自保存不同的**一部分参数**。大家合起来才构成完整模型。不同颜色一般是用来区分**同一个大批次（batch）里被切分开的不同子块**。所有颜色合起来才是**同一次 forward/backward 的整个 batch**   ，换句话说，整张图中的所有颜色块加起来才构成一次完整的 batch；在实际训练中，我们往往会在每个训练 step 中取一个大的 batch，然后按照硬件并行策略把它拆分给各核心，各核心并行处理后再在梯度或激活等环节进行必要的通信与汇总。
- **Expert and Data Parallelism**：每个小颜色方块可以看作一个**专家（Expert）对应的参数 ** ，但“非专家”部分（如自注意力层、嵌入层等）通常会完整复制，

## Adapters tuning







## 上手代码transformer


ner_datasets["train"].features 可以查看数据集的特征