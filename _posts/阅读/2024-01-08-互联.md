---
layout: post
title: 互联
date: 2025-1-8 19:22 +0800
categories: [读书笔记, 资料检索]
tags: []
math: true
img_path: /assets/img/learn/interconnect
---

## 一些概念

### NCCL互联方案
在常见的分布式训练加速设备中，常常是多节点多加速卡的形式，节点也可被称之为主机或CPU，加速卡的种类很多，常见的有GPU、DCU、FPGA等。如下图所示
- 在单节点多加速卡的情况下，节点和加速卡以及加速卡之间的数据通信依靠PCIe或NVLink实现
- 多节点多加速卡的情况下，节点之间的数据通信依靠`以太网或Infiniband`实现。

![多节点通信方案](https://i-blog.csdnimg.cn/blog_migrate/67346946f0c2d85b1f7a31e1dbecf670.png#pic_center)  

在跨节点加速卡通信的过程中，往往需要先将加速卡的数据传输到相应节点的CPU上，然后CPU通过以太网传输数据，之后又将数据传给加速卡，这种数据在节点和加速卡之间频繁移动所造成的通信开销是很大的，鉴于此，英伟达公司发布了 GPU Direct技术，用于提高加速卡之间通信的效率。

 *  在单节点多加速卡通信中，提出`P2P（GPU Direct peer-to-peer）`技术。如下图（a）（b）所示，它实现了节点内部加速卡的直接通信，即加速卡可以直接访问另一个加速卡的内存并实现数据的直接传输，避免了加速卡的数据复制到节点CPU内存上作为中转。
 *  在多节点多加速卡通信中，提出了`GDR(GPU direct RDMA)`技术，如下图（c）所示，加速卡和网卡可以直接通过PCIe进行数据交互，避免了跨节点通信过程中内存和CPU的参与。从而实现加速卡可以直接访问其他节点的加速卡内存。  
    ![通信中的直连设计](https://i-blog.csdnimg.cn/blog_migrate/439954a04df1e52efeea8ae9d4801f25.png#pic_center)

### 通信原语Collective communication

* Reduce：从多个sender那里接收数据，最终combine到一个节点上面
* All-reduce：从多个sender那里接收数据，最终combine到每一个节点上面
* #传统的TCP/IP通信 
> 1. 数据发送方要将数据从`用户空间`Buffer`复制到内核空间`的Socket Buffer中。
> 2. 在`内核`空间中`添加数据报头`，进行数据封装。通过一系列多层网络协议的数据包处理工作，这些协议包括传输控制协议（TCP）、用户数据报协议（UDP）、互联网协议（IP）、以及互联网控制消息协议（ICMP）等。经历如此多个步骤，数据才能`被Push到NIC网卡中的Buffer`进行网络传输
> 3. 在消息接收方，从远程主机发送来的数据包，要先将其`从NIC Buffer拷贝至Socket Buffer`
> 4. 经过一系列的多层网络协议对数据包进行解析，解析后的`数据被复制到相应的用户空间应用程序的Buffer中`。此时，再进行系统上下文切换，用户应用程序才被调用。
{: .prompt-tip}
* #TCP/IP存在的问题 
> 传统的TCP/IP网络通信是`通过内核发送消息`. 需要在内核中频繁进行协议封装和解封操作，造成很大的数据移动和数据复制开销。RDMA提供了给基于IO的通道，这种通道允许一个应用程序通过RDMA设备对远程的虚拟内存进行直接的读写。
> 目前，有三种支持RDMA的通信技术：`IB(InfiniBand)`、`以太网RoCE(RDMA over Converged Ethernet)`、`以太网iWARP(internet Wide Area RDMA Protocal)`.它们有着不同的物理层和链路层。
{: .prompt-tip}

### Allgather vs Alltoall

![Allgather]({{ page.img_path }}1.png){: width="972" height="589" }
![Alltoall]({{ page.img_path }}2.png){: width="972" height="589" }

简单分析发送缓冲区中数据的大小和接收缓冲区中数据的大小：
```
operation      send buf size      recv buf size 
---------      -------------      ------------- 
MPI_Allgather     sendcnt        n_procs * sendcnt 
MPI_Alltoall  n_procs * sendcnt  n_procs * sendcnt
```


Ring算法在中等规模的运算中非常有优势，较小的传输数据量，无瓶颈，带宽完全利用起来。  
缺点则是在大型规模集群运算中，巨大的服务器内数据，极长的Ring环，Ring的这种切分数据块的方式就不再占优势。